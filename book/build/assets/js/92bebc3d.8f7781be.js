"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[617],{3613(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"part-5-vision-language-action/chapter-10-building-vla-systems","title":"Chapter 10: Building a Vision-Language-Action System","description":"This chapter brings together the concepts of perception, language understanding, and robotic action to construct a complete Vision-Language-Action (VLA) system. A VLA system enables a robot to interpret natural language commands, understand its visual environment, and perform appropriate physical actions.","source":"@site/docs/part-5-vision-language-action/chapter-10-building-vla-systems.md","sourceDirName":"part-5-vision-language-action","slug":"/part-5-vision-language-action/chapter-10-building-vla-systems","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-5-vision-language-action/chapter-10-building-vla-systems","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/humanoid-robotics-book/tree/main/docs/part-5-vision-language-action/chapter-10-building-vla-systems.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"sidebar_position":10},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 9: Connecting LLMs to ROS 2","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-5-vision-language-action/chapter-9-connecting-llms-to-ros"},"next":{"title":"Chapter 11: The Capstone Project","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-6-capstone-project/chapter-11-autonomous-conversational-humanoid"}}');var i=t(4848),a=t(8453);const s={sidebar_position:10},r="Chapter 10: Building a Vision-Language-Action System",c={},l=[{value:"Integrating a Vision Model",id:"integrating-a-vision-model",level:2},{value:"Vision Model Workflow:",id:"vision-model-workflow",level:3},{value:"Creating a VLA Pipeline",id:"creating-a-vla-pipeline",level:2},{value:"Conceptual VLA Architecture:",id:"conceptual-vla-architecture",level:3},{value:"End-to-End Example",id:"end-to-end-example",level:2},{value:"Pipeline Steps:",id:"pipeline-steps",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-10-building-a-vision-language-action-system",children:"Chapter 10: Building a Vision-Language-Action System"})}),"\n",(0,i.jsx)(n.p,{children:"This chapter brings together the concepts of perception, language understanding, and robotic action to construct a complete Vision-Language-Action (VLA) system. A VLA system enables a robot to interpret natural language commands, understand its visual environment, and perform appropriate physical actions."}),"\n",(0,i.jsx)(n.h2,{id:"integrating-a-vision-model",children:"Integrating a Vision Model"}),"\n",(0,i.jsx)(n.p,{children:"The first step in building a VLA system is to integrate a robust vision model that can interpret the robot's sensory input. This model will typically process camera images to identify objects, their locations, and other relevant environmental features."}),"\n",(0,i.jsx)(n.h3,{id:"vision-model-workflow",children:"Vision Model Workflow:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Image Acquisition"}),": Obtain real-time images from the robot's camera (e.g., via a ROS 2 image topic)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perception Node"}),": A dedicated ROS 2 node runs the vision model (e.g., an object detection model like YOLO, a segmentation model)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output to World Model"}),": The perception node publishes its findings (e.g., object bounding boxes, semantic labels, 3D poses) to a ROS 2 topic or service, which can then be integrated into the robot's world model."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"creating-a-vla-pipeline",children:"Creating a VLA Pipeline"}),"\n",(0,i.jsx)(n.p,{children:"A VLA pipeline orchestrates the flow of information between the language understanding, visual perception, and action generation components."}),"\n",(0,i.jsx)(n.h3,{id:"conceptual-vla-architecture",children:"Conceptual VLA Architecture:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:"graph TD\r\n    A[Natural Language Command] --\x3e B(LLM Interface Node);\r\n    B --\x3e C{LLM (e.g., OpenAI)};\r\n    C --\x3e D[Parsed Intent/Action Plan];\r\n    D --\x3e E[Action Planner Node];\r\n    E --\x3e F{Robot World Model (Perception Data)};\r\n    F --\x3e G[Motion/Manipulation Controller Node];\r\n    H[Camera Sensor] --\x3e I(Vision Perception Node);\r\n    I --\x3e F;\r\n    G --\x3e J[Robot Actuators];\r\n    J --\x3e K[Physical Robot];\n"})}),"\n",(0,i.jsx)(n.h2,{id:"end-to-end-example",children:"End-to-End Example"}),"\n",(0,i.jsx)(n.p,{children:'We will walk through an example where a robot is commanded to "pick up the red cube from the table."'}),"\n",(0,i.jsx)(n.h3,{id:"pipeline-steps",children:"Pipeline Steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"User Command"}),': A human says "pick up the red cube from the table." This command is captured by a speech-to-text system (not covered in detail here) and published as a natural language command to the ',(0,i.jsx)(n.code,{children:"natural_language_command"})," topic."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLM Interpretation"}),": The ",(0,i.jsx)(n.code,{children:"LLMInterfaceNode"})," (from Chapter 9) receives the command, queries the LLM, and gets an action plan like ",(0,i.jsx)(n.code,{children:'{"action": "pick_object", "object": "red cube", "location": "table"}'}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Planning"}),": An ",(0,i.jsx)(n.code,{children:"ActionPlannerNode"})," receives this parsed intent. It then queries the ",(0,i.jsx)(n.code,{children:"RobotWorldModel"})," (which is updated by the ",(0,i.jsx)(n.code,{children:"VisionPerceptionNode"}),') to find the 3D pose of the "red cube" on the "table."']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Motion Generation"}),": With the object's pose, the ",(0,i.jsx)(n.code,{children:"ActionPlannerNode"})," requests the ",(0,i.jsx)(n.code,{children:"MotionControllerNode"})," (e.g., using MoveIt 2) to plan and execute a grasping motion."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Execution"}),": The ",(0,i.jsx)(n.code,{children:"MotionControllerNode"})," sends commands to the ",(0,i.jsx)(n.code,{children:"RobotActuators"}),", and the physical robot picks up the cube."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback"}),': The robot can provide feedback to the user (e.g., "I have picked up the red cube") via a text-to-speech system.']}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>r});var o=t(6540);const i={},a=o.createContext(i);function s(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);