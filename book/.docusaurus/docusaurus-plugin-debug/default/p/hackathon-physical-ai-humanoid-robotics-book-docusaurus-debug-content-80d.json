{"allContent":{"docusaurus-plugin-css-cascade-layers":{},"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs","tagsPath":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/tags","editUrl":"https://github.com/Atiq-Developer/Hackathon_Physical_AI_-Humanoid_Robotics_Book/tree/main/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"D:\\ai_book_physical\\book\\sidebars.ts","contentPath":"D:\\ai_book_physical\\book\\docs","docs":[{"id":"intro","title":"Physical AI & Humanoid Robotics","description":"Welcome to the Physical AI & Humanoid Robotics textbook.","source":"@site/docs/intro.md","sourceDirName":".","slug":"/intro","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/Atiq-Developer/Hackathon_Physical_AI_-Humanoid_Robotics_Book/tree/main/docs/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"slug":"/intro"},"sidebar":"tutorialSidebar","next":{"title":"Chapter 1: Introduction to Physical AI","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-1-foundations/chapter-1-intro-to-physical-ai"}},{"id":"part-1-foundations/chapter-1-intro-to-physical-ai","title":"Chapter 1: Introduction to Physical AI","description":"Welcome to the exciting world of Physical AI! This chapter will introduce you to the fundamental concepts that bridge the gap between the digital realm of artificial intelligence and the physical world of robotics.","source":"@site/docs/part-1-foundations/chapter-1-intro-to-physical-ai.md","sourceDirName":"part-1-foundations","slug":"/part-1-foundations/chapter-1-intro-to-physical-ai","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-1-foundations/chapter-1-intro-to-physical-ai","draft":false,"unlisted":false,"editUrl":"https://github.com/Atiq-Developer/Hackathon_Physical_AI_-Humanoid_Robotics_Book/tree/main/docs/part-1-foundations/chapter-1-intro-to-physical-ai.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Physical AI & Humanoid Robotics","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/intro"},"next":{"title":"Chapter 2: Embodied Intelligence","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-1-foundations/chapter-2-embodied-intelligence"}},{"id":"part-1-foundations/chapter-2-embodied-intelligence","title":"Chapter 2: Embodied Intelligence","description":"This chapter explores the concept of embodied intelligence, a cornerstone of modern robotics and Physical AI. We will delve into how a physical body shapes the nature of intelligence and how robots perceive and interact with their environment.","source":"@site/docs/part-1-foundations/chapter-2-embodied-intelligence.md","sourceDirName":"part-1-foundations","slug":"/part-1-foundations/chapter-2-embodied-intelligence","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-1-foundations/chapter-2-embodied-intelligence","draft":false,"unlisted":false,"editUrl":"https://github.com/Atiq-Developer/Hackathon_Physical_AI_-Humanoid_Robotics_Book/tree/main/docs/part-1-foundations/chapter-2-embodied-intelligence.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Introduction to Physical AI","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-1-foundations/chapter-1-intro-to-physical-ai"},"next":{"title":"Chapter 3: ROS 2 Architecture","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-2-ros-2/chapter-3-ros-2-architecture"}},{"id":"part-2-ros-2/chapter-3-ros-2-architecture","title":"Chapter 3: ROS 2 Architecture","description":"This chapter dives into the heart of modern robotics development: the Robot Operating System 2 (ROS 2). We'll explore its fundamental architecture, key concepts, and how it enables complex robotic systems to communicate and coordinate.","source":"@site/docs/part-2-ros-2/chapter-3-ros-2-architecture.md","sourceDirName":"part-2-ros-2","slug":"/part-2-ros-2/chapter-3-ros-2-architecture","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-2-ros-2/chapter-3-ros-2-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/Atiq-Developer/Hackathon_Physical_AI_-Humanoid_Robotics_Book/tree/main/docs/part-2-ros-2/chapter-3-ros-2-architecture.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Embodied Intelligence","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-1-foundations/chapter-2-embodied-intelligence"},"next":{"title":"Chapter 4: Building a ROS 2 System","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-2-ros-2/chapter-4-ros-2-nodes-and-messages"}},{"id":"part-2-ros-2/chapter-4-ros-2-nodes-and-messages","title":"Chapter 4: Building a ROS 2 System","description":"Building a ROS 2 system involves creating workspaces, packages, and custom message types to facilitate communication between your robot's components. This chapter will walk you through the practical steps of setting up your development environment and creating your first ROS 2 nodes.","source":"@site/docs/part-2-ros-2/chapter-4-ros-2-nodes-and-messages.md","sourceDirName":"part-2-ros-2","slug":"/part-2-ros-2/chapter-4-ros-2-nodes-and-messages","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-2-ros-2/chapter-4-ros-2-nodes-and-messages","draft":false,"unlisted":false,"editUrl":"https://github.com/Atiq-Developer/Hackathon_Physical_AI_-Humanoid_Robotics_Book/tree/main/docs/part-2-ros-2/chapter-4-ros-2-nodes-and-messages.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: ROS 2 Architecture","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-2-ros-2/chapter-3-ros-2-architecture"},"next":{"title":"Chapter 5: Simulation-First Development","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-3-simulation/chapter-5-simulation-first-development"}},{"id":"part-3-simulation/chapter-5-simulation-first-development","title":"Chapter 5: Simulation-First Development","description":"In the rapidly evolving field of robotics, the ability to rapidly prototype, test, and iterate on complex systems is paramount. Simulation-first development has emerged as a critical methodology, allowing engineers and researchers to design and validate robotic behaviors in a virtual environment before deploying them to costly and potentially dangerous physical hardware.","source":"@site/docs/part-3-simulation/chapter-5-simulation-first-development.md","sourceDirName":"part-3-simulation","slug":"/part-3-simulation/chapter-5-simulation-first-development","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-3-simulation/chapter-5-simulation-first-development","draft":false,"unlisted":false,"editUrl":"https://github.com/Atiq-Developer/Hackathon_Physical_AI_-Humanoid_Robotics_Book/tree/main/docs/part-3-simulation/chapter-5-simulation-first-development.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Building a ROS 2 System","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-2-ros-2/chapter-4-ros-2-nodes-and-messages"},"next":{"title":"Chapter 6: Simulating Robots with Gazebo and Unity","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-3-simulation/chapter-6-gazebo-and-unity-for-robotics"}},{"id":"part-3-simulation/chapter-6-gazebo-and-unity-for-robotics","title":"Chapter 6: Simulating Robots with Gazebo and Unity","description":"This chapter provides practical guidance on how to simulate robots using two popular platforms: Gazebo and Unity. We will cover the creation of robot models, their integration into these simulators, and basic interaction techniques.","source":"@site/docs/part-3-simulation/chapter-6-gazebo-and-unity-for-robotics.md","sourceDirName":"part-3-simulation","slug":"/part-3-simulation/chapter-6-gazebo-and-unity-for-robotics","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-3-simulation/chapter-6-gazebo-and-unity-for-robotics","draft":false,"unlisted":false,"editUrl":"https://github.com/Atiq-Developer/Hackathon_Physical_AI_-Humanoid_Robotics_Book/tree/main/docs/part-3-simulation/chapter-6-gazebo-and-unity-for-robotics.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: Simulation-First Development","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-3-simulation/chapter-5-simulation-first-development"},"next":{"title":"Chapter 7: Perception with NVIDIA Isaac","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-4-perception-and-control/chapter-7-nvidia-isaac-for-perception"}},{"id":"part-4-perception-and-control/chapter-7-nvidia-isaac-for-perception","title":"Chapter 7: Perception with NVIDIA Isaac","description":"NVIDIA Isaac Sim, built on the Omniverse platform, is a powerful robotics simulation and synthetic data generation tool that accelerates the development of AI-powered robots. This chapter will focus on leveraging Isaac Sim for perception tasks, from simulating sensors to generating synthetic datasets for training deep learning models.","source":"@site/docs/part-4-perception-and-control/chapter-7-nvidia-isaac-for-perception.md","sourceDirName":"part-4-perception-and-control","slug":"/part-4-perception-and-control/chapter-7-nvidia-isaac-for-perception","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-4-perception-and-control/chapter-7-nvidia-isaac-for-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Atiq-Developer/Hackathon_Physical_AI_-Humanoid_Robotics_Book/tree/main/docs/part-4-perception-and-control/chapter-7-nvidia-isaac-for-perception.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 6: Simulating Robots with Gazebo and Unity","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-3-simulation/chapter-6-gazebo-and-unity-for-robotics"},"next":{"title":"Chapter 8: Control Pipelines","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-4-perception-and-control/chapter-8-control-pipelines"}},{"id":"part-4-perception-and-control/chapter-8-control-pipelines","title":"Chapter 8: Control Pipelines","description":"Effective robotic control is the bridge between perceiving the world and acting upon it. This chapter delves into the fundamental concepts of robotic control pipelines, covering how robots execute motions, maintain stability, and interact with their environment. We will also explore advanced tools like MoveIt 2 for complex motion planning.","source":"@site/docs/part-4-perception-and-control/chapter-8-control-pipelines.md","sourceDirName":"part-4-perception-and-control","slug":"/part-4-perception-and-control/chapter-8-control-pipelines","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-4-perception-and-control/chapter-8-control-pipelines","draft":false,"unlisted":false,"editUrl":"https://github.com/Atiq-Developer/Hackathon_Physical_AI_-Humanoid_Robotics_Book/tree/main/docs/part-4-perception-and-control/chapter-8-control-pipelines.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 7: Perception with NVIDIA Isaac","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-4-perception-and-control/chapter-7-nvidia-isaac-for-perception"},"next":{"title":"Chapter 9: Connecting LLMs to ROS 2","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-5-vision-language-action/chapter-9-connecting-llms-to-ros"}},{"id":"part-5-vision-language-action/chapter-10-building-vla-systems","title":"Chapter 10: Building a Vision-Language-Action System","description":"This chapter brings together the concepts of perception, language understanding, and robotic action to construct a complete Vision-Language-Action (VLA) system. A VLA system enables a robot to interpret natural language commands, understand its visual environment, and perform appropriate physical actions.","source":"@site/docs/part-5-vision-language-action/chapter-10-building-vla-systems.md","sourceDirName":"part-5-vision-language-action","slug":"/part-5-vision-language-action/chapter-10-building-vla-systems","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-5-vision-language-action/chapter-10-building-vla-systems","draft":false,"unlisted":false,"editUrl":"https://github.com/Atiq-Developer/Hackathon_Physical_AI_-Humanoid_Robotics_Book/tree/main/docs/part-5-vision-language-action/chapter-10-building-vla-systems.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"sidebar_position":10},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 9: Connecting LLMs to ROS 2","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-5-vision-language-action/chapter-9-connecting-llms-to-ros"},"next":{"title":"Chapter 11: The Capstone Project","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-6-capstone-project/chapter-11-autonomous-conversational-humanoid"}},{"id":"part-5-vision-language-action/chapter-9-connecting-llms-to-ros","title":"Chapter 9: Connecting LLMs to ROS 2","description":"Integrating Large Language Models (LLMs) with ROS 2 allows robots to understand natural language commands and generate complex action sequences. This chapter explores how to bridge the gap between an LLM's cognitive abilities and a robot's physical actions within the ROS 2 framework.","source":"@site/docs/part-5-vision-language-action/chapter-9-connecting-llms-to-ros.md","sourceDirName":"part-5-vision-language-action","slug":"/part-5-vision-language-action/chapter-9-connecting-llms-to-ros","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-5-vision-language-action/chapter-9-connecting-llms-to-ros","draft":false,"unlisted":false,"editUrl":"https://github.com/Atiq-Developer/Hackathon_Physical_AI_-Humanoid_Robotics_Book/tree/main/docs/part-5-vision-language-action/chapter-9-connecting-llms-to-ros.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 8: Control Pipelines","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-4-perception-and-control/chapter-8-control-pipelines"},"next":{"title":"Chapter 10: Building a Vision-Language-Action System","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-5-vision-language-action/chapter-10-building-vla-systems"}},{"id":"part-6-capstone-project/chapter-11-autonomous-conversational-humanoid","title":"Chapter 11: The Capstone Project","description":"This chapter is the culmination of your journey through Physical AI and humanoid robotics. You will integrate the knowledge and skills acquired throughout the book to design and implement an autonomous conversational humanoid robot in a simulated environment. The goal is to build a system that can understand natural language commands, perceive its surroundings, and perform complex actions.","source":"@site/docs/part-6-capstone-project/chapter-11-autonomous-conversational-humanoid.md","sourceDirName":"part-6-capstone-project","slug":"/part-6-capstone-project/chapter-11-autonomous-conversational-humanoid","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-6-capstone-project/chapter-11-autonomous-conversational-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/Atiq-Developer/Hackathon_Physical_AI_-Humanoid_Robotics_Book/tree/main/docs/part-6-capstone-project/chapter-11-autonomous-conversational-humanoid.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"sidebar_position":11},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 10: Building a Vision-Language-Action System","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-5-vision-language-action/chapter-10-building-vla-systems"},"next":{"title":"Chapter 12: Future Directions","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-6-capstone-project/chapter-12-future-directions"}},{"id":"part-6-capstone-project/chapter-12-future-directions","title":"Chapter 12: Future Directions","description":"Congratulations on completing the capstone project and working through this book! The field of Physical AI and humanoid robotics is incredibly dynamic, with new breakthroughs emerging constantly. This final chapter will briefly touch upon some exciting future directions and provide guidance on how to stay engaged and continue learning.","source":"@site/docs/part-6-capstone-project/chapter-12-future-directions.md","sourceDirName":"part-6-capstone-project","slug":"/part-6-capstone-project/chapter-12-future-directions","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-6-capstone-project/chapter-12-future-directions","draft":false,"unlisted":false,"editUrl":"https://github.com/Atiq-Developer/Hackathon_Physical_AI_-Humanoid_Robotics_Book/tree/main/docs/part-6-capstone-project/chapter-12-future-directions.md","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"sidebar_position":12},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 11: The Capstone Project","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/docs/part-6-capstone-project/chapter-11-autonomous-conversational-humanoid"}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"doc","id":"intro"},{"type":"category","label":"part-1-foundations","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"part-1-foundations/chapter-1-intro-to-physical-ai"},{"type":"doc","id":"part-1-foundations/chapter-2-embodied-intelligence"}]},{"type":"category","label":"part-2-ros-2","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"part-2-ros-2/chapter-3-ros-2-architecture"},{"type":"doc","id":"part-2-ros-2/chapter-4-ros-2-nodes-and-messages"}]},{"type":"category","label":"part-3-simulation","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"part-3-simulation/chapter-5-simulation-first-development"},{"type":"doc","id":"part-3-simulation/chapter-6-gazebo-and-unity-for-robotics"}]},{"type":"category","label":"part-4-perception-and-control","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"part-4-perception-and-control/chapter-7-nvidia-isaac-for-perception"},{"type":"doc","id":"part-4-perception-and-control/chapter-8-control-pipelines"}]},{"type":"category","label":"part-5-vision-language-action","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"part-5-vision-language-action/chapter-9-connecting-llms-to-ros"},{"type":"doc","id":"part-5-vision-language-action/chapter-10-building-vla-systems"}]},{"type":"category","label":"part-6-capstone-project","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"part-6-capstone-project/chapter-11-autonomous-conversational-humanoid"},{"type":"doc","id":"part-6-capstone-project/chapter-12-future-directions"}]}]}}]}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/","source":"@site/src/pages/index.tsx"},{"type":"mdx","permalink":"/Hackathon_Physical_AI_-Humanoid_Robotics_Book/markdown-page","source":"@site/src/pages/markdown-page.md","title":"Markdown page example","description":"You don't need React to write simple standalone pages.","frontMatter":{"title":"Markdown page example"},"unlisted":false}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}